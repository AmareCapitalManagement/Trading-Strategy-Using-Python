{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4696d294-10f3-4e89-b10c-c948ae9c26a8",
   "metadata": {},
   "source": [
    "### QUANTITATIVE VALUE AND MOMENTUM FACTOR STRATEGY\n",
    "\n",
    "Value factor identifies stocks trading below their intrinsic value, using metrics like price-to-earnings (P/E), price-to-book (P/B), price-to-sales (P/S), enterprise value to EBITDA (EV/EBITDA), and enterprise value to gross profit (EV/GP). These metrics evaluate a stock's price relative to earnings, assets, sales, or profitability. Momentum factor focuses on stocks with the highest recent price momentum, assuming that stocks that have been performing well recently will continue to perform well in the short-term. This strategy targets stocks with the lowest valuation metrics, assuming they are undervalued and likely to outperform and exhibit strong, high-quality momentum across various timeframes (1-month, 3-months, 6-months, and 1-year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e121b817-c03b-44e8-a993-9a8fa5301034",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import math\n",
    "from scipy import stats \n",
    "import warnings\n",
    "from statistics import mean \n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "stocks = [\"ABG.JO\", \"ADH.JO\", \"AEL.JO\", \"AFE.JO\", \"AFH.JO\", \"AFT.JO\", \"AGL.JO\", \"AHR.JO\", \"AIP.JO\", \"ANG.JO\", \"ANH.JO\", \"APN.JO\", \"ARI.JO\",\n",
    "          \"ARL.JO\", \"ATT.JO\", \"AVI.JO\", \"BAW.JO\", \"BHG.JO\", \"BID.JO\", \"BLU.JO\", \"BOX.JO\", \"BTI.JO\", \"BTN.JO\", \"BVT.JO\", \"BYI.JO\", \"CFR.JO\", \"CLS.JO\",\n",
    "          \"CML.JO\", \"COH.JO\", \"CPI.JO\", \"CSB.JO\", \"DCP.JO\", \"DRD.JO\", \"DSY.JO\", \"DTC.JO\", \"EMI.JO\", \"EQU.JO\", \"EXX.JO\", \"FBR.JO\", \"FFB.JO\", \"FSR.JO\",\n",
    "          \"FTB.JO\", \"GFI.JO\", \"GLN.JO\", \"GND.JO\", \"GRT.JO\", \"HAR.JO\", \"HCI.JO\", \"HDC.JO\", \"HMN.JO\", \"HYP.JO\", \"IMP.JO\", \"INL.JO\", \"INP.JO\", \"ITE.JO\",\n",
    "          \"JSE.JO\", \"KAP.JO\", \"KIO.JO\", \"KRO.JO\", \"KST.JO\", \"LHC.JO\", \"LTE.JO\", \"MCG.JO\", \"MKR.JO\", \"MNP.JO\", \"MRP.JO\", \"MSP.JO\", \"MTH.JO\", \"MTM.JO\",\n",
    "          \"MTN.JO\", \"N91.JO\", \"NED.JO\", \"NPH.JO\", \"NPN.JO\", \"NRP.JO\", \"NTC.JO\", \"NY1.JO\", \"OCE.JO\", \"OMN.JO\", \"OMU.JO\", \"OUT.JO\", \"PAN.JO\", \"PHP.JO\",\n",
    "          \"PIK.JO\", \"PMR.JO\", \"PPC.JO\", \"PPH.JO\", \"PRX.JO\", \"QLT.JO\", \"RBX.JO\", \"RCL.JO\", \"RDF.JO\", \"REM.JO\", \"RES.JO\", \"RLO.JO\", \"RNI.JO\", \"S32.JO\",\n",
    "          \"SAC.JO\", \"SAP.JO\", \"SBK.JO\", \"SHC.JO\", \"SHP.JO\", \"SLM.JO\", \"SNT.JO\", \"SOL.JO\", \"SPG.JO\", \"SPP.JO\", \"SRE.JO\", \"SRI.JO\", \"SSS.JO\",\"SSU.JO\", \n",
    "          \"SSW.JO\", \"SUI.JO\", \"TBS.JO\", \"TFG.JO\", \"TGA.JO\", \"TKG.JO\", \"TRU.JO\", \"TSG.JO\", \"VAL.JO\", \"VKE.JO\", \"VOD.JO\", \"WBC.JO\", \"WHL.JO\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243da1d8-16a7-49ba-848b-d196abd59e82",
   "metadata": {},
   "source": [
    "**Explanation**\n",
    "\n",
    "We use numpy for calculations, pandas for data handling, yfinance for Yahoo Finance data, math for share calculations, scipy.stats for percentiles, and xlswriter for Excel output. The warnings library suppress yfinance depreciation warnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bfc0a4-a752-4e3c-ada4-9fec659d9241",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display \n",
    "\n",
    "def get_valuation_ratios(ticker):\n",
    "    stock = yf.Ticker(ticker)\n",
    "    info = stock.info\n",
    "\n",
    "    try:\n",
    "        name = info.get('shortName', 'N/A')\n",
    "        sector = info.get('sector', 'N/A')\n",
    "        price_cents = info.get('currentPrice', np.nan)\n",
    "        price = price_cents / 100 if not np.isnan(price_cents) else np.nan\n",
    "        price_str = f\"R{price:,.2f}\" if not np.isnan(price) else \"N/A\"\n",
    "        \n",
    "        pe_ratio = info.get('trailingPE', np.nan)\n",
    "        pb_ratio = info.get('priceToBook', np.nan)\n",
    "        if np.isnan(pb_ratio):\n",
    "            print(f\"P/B is missing for {ticker}\")\n",
    "        ps_ratio = info.get('priceToSalesTrailing12Months', np.nan)\n",
    "        ev = info.get('enterpriseValue', np.nan)\n",
    "        ebitda = info.get('ebitda', np.nan)\n",
    "        gross_profit = info.get('grossProfits', np.nan)\n",
    "        ev_to_ebitda = ev / ebitda if ev and ebitda else np.nan\n",
    "        ev_to_gp = ev / gross_profit if ev and gross_profit else np.nan\n",
    "\n",
    "        return {\n",
    "            'Ticker': ticker,\n",
    "            'Name': name,\n",
    "            'Sector': sector,\n",
    "            'Price': price_str,\n",
    "            'P/E': pe_ratio,\n",
    "            'P/B': pb_ratio,\n",
    "            'P/S': ps_ratio,\n",
    "            'EV/EBITDA': ev_to_ebitda,\n",
    "            'EV/GP': ev_to_gp\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching ratios for {ticker}: {e}\")\n",
    "        return {\n",
    "            'Ticker': ticker,\n",
    "            'Name': 'N/A',\n",
    "            'Sector':'N/A',\n",
    "            'Price': 'N/A',\n",
    "            'P/E': np.nan,\n",
    "            'P/B': np.nan,\n",
    "            'P/S': np.nan,\n",
    "            'EV/EBITDA': np.nan,\n",
    "            'EV/GP': np.nan\n",
    "        }\n",
    "\n",
    "value_data = [get_valuation_ratios(ticker) for ticker in stocks]\n",
    "value_df = pd.DataFrame(value_data)\n",
    "\n",
    "value_metrics = ['P/E', 'P/B', 'P/S', 'EV/EBITDA', 'EV/GP']\n",
    "for col in value_metrics:\n",
    "    value_df[col] = pd.to_numeric(value_df[col], errors='coerce').round(0).astype('Int64')\n",
    "\n",
    "end_date = datetime.today()\n",
    "start_date = end_date - timedelta(days=730)\n",
    "\n",
    "price_data = yf.download(stocks, start=start_date, end=end_date)['Close']\n",
    "momentum_df = pd.DataFrame(index=stocks)\n",
    "\n",
    "lookback_periods = {\n",
    "    '1M': 21,\n",
    "    '3M': 63,\n",
    "    '6M': 126,\n",
    "    '1Y': 252,\n",
    "}\n",
    "\n",
    "for label, days in lookback_periods.items():\n",
    "    returns = price_data.pct_change(periods=days).iloc[-1]\n",
    "    momentum_df[f\"{label} Return\"] = returns\n",
    "\n",
    "for label in lookback_periods.keys():\n",
    "    momentum_df[f\"{label} Return\"] = momentum_df[f\"{label} Return\"].apply(lambda x: f\"{x:.2%}\")\n",
    "\n",
    "combined_df = pd.merge(value_df, momentum_df, left_on='Ticker', right_index=True)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "display(combined_df)\n",
    "\n",
    "combined_df.to_excel(\"stock_valuation_momentum.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d387c6f-ecc8-4986-9d35-aee9677b9039",
   "metadata": {},
   "source": [
    "**Explanation**\n",
    "\n",
    "We use yfinance to fetch stock data (price, trailing P/E, P/B, P/S, enterprise value, EBIT, EBITDA, gross profit) and calculate EV/EBIT, EV/EBITDA, and EV/GP. A DataFrame is initialized with columns for tickers, prices, shares to buy, metrics, percentiles, and a robust value (RV) score. Error handling ensures robustness if data is missing. Combining data fetching and DataFrame creation streamlines the data collection process. Also, the historical price data will allow us to calculate momentum returns over different time periods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393d80fe-4ca9-4281-8698-1e3618e961b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "valuation_columns=['P/E', 'P/B', 'P/S','EV/EBITDA', 'EV/GP']\n",
    "\n",
    "momentum_columns = ['1M Return','3M Return','6M Return','1Y Return']\n",
    "\n",
    "for column in valuation_columns:\n",
    "    combined_df[column] = combined_df[column].astype(float)\n",
    "    combined_df[column].fillna(combined_df[column].mean(), inplace=True)\n",
    "\n",
    "for column in momentum_columns:\n",
    "    combined_df[column] = combined_df[column].str.rstrip('%').astype(float) / 100\n",
    "    combined_df[column].fillna(combined_df[column].mean(), inplace=True)\n",
    "\n",
    "print(\"Missing values per column:\")\n",
    "print(combined_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3cec3b-4dcd-47f7-8e4d-54577cac8cf4",
   "metadata": {},
   "source": [
    "**Explanation**\n",
    "\n",
    "We replace missing values with the mean of non-missing values for each metric. This preserves the dataset's size, assuming missing data is not systematically biased. This step remains standalone due to its distinct focus on data cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d26fdc-94d5-44f8-92b9-5eb733478827",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_metrics = {\n",
    "    'P/E': 'PE Percentile',\n",
    "    'P/B': 'PB Percentile',\n",
    "    'P/S': 'PS Percentile',\n",
    "    'EV/EBITDA': 'EV/EBITDA Percentile',\n",
    "    'EV/GP': 'EV/GP Percentile'\n",
    "}\n",
    "\n",
    "momentum_metrics = {\n",
    "    '1M Return': '1M Percentile',\n",
    "    '3M Return': '3M Percentile',\n",
    "    '6M Return': '6M Percentile',\n",
    "    '1Y Return': '1Y Percentile'\n",
    "}\n",
    "\n",
    "for row in combined_df.index:\n",
    "    for metric, pct_col in value_metrics.items():\n",
    "        combined_df.loc[row, pct_col] = stats.percentileofscore(\n",
    "            combined_df[metric], combined_df.loc[row, metric]\n",
    "        ) / 100\n",
    "\n",
    "for row in combined_df.index:\n",
    "    for metric, pct_col in momentum_metrics.items():\n",
    "        combined_df.loc[row, pct_col] = stats.percentileofscore(\n",
    "            combined_df[metric], combined_df.loc[row, metric]\n",
    "        ) / 100\n",
    "\n",
    "combined_df['Value Score'] = combined_df[list(value_metrics.values())].mean(axis=1)\n",
    "combined_df['Momentum Score'] = combined_df[list(momentum_metrics.values())].mean(axis=1)\n",
    "\n",
    "value_buy_thresh = combined_df['Value Score'].quantile(0.2)\n",
    "value_sell_thresh = combined_df['Value Score'].quantile(0.8)\n",
    "momentum_buy_thresh = combined_df['Momentum Score'].quantile(0.8)\n",
    "momentum_sell_thresh = combined_df['Momentum Score'].quantile(0.2)\n",
    "\n",
    "combined_df['Value Signal'] = combined_df['Value Score'].apply(\n",
    "    lambda x: 'BUY' if x <= value_buy_thresh else ('SELL' if x >= value_sell_thresh else 'HOLD')\n",
    ")\n",
    "\n",
    "combined_df['Momentum Signal'] = combined_df['Momentum Score'].apply(\n",
    "    lambda x: 'BUY' if x >= momentum_buy_thresh else ('SELL' if x <= momentum_sell_thresh else 'HOLD')\n",
    ")\n",
    "\n",
    "final_df = combined_df[['Ticker', 'Value Score', 'Value Signal', 'Momentum Score', 'Momentum Signal']]\n",
    "final_df.sort_values(by='Ticker', inplace=True)\n",
    "final_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "print(final_df) \n",
    "\n",
    "final_df.to_excel(\"value_momentum_signals.xlsx\", index=False)                                                                                        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da98446-c3f9-49a8-9477-9822e19e554a",
   "metadata": {},
   "source": [
    "**Explanation**\n",
    "\n",
    "We compute percentile ranks for each metric using scipy.stats.percentile of score.  The RV Score is the average of the valuation metrics, with lower scores indicating better value, and the average of the momentum metrics, with higher scores indicating higher quality momentum. We sort by RV Score, select the top stocks, and reset the index. Combining these steps aligns the analytical focus on ranking and filtering. \n",
    "\n",
    "The quantitative value and momentum factor strategy is designed to identify and capitalize on two distinct market inefficiencies: undervaluation (via the value factor) and price trends (via the momentum factor). The screener aims to buy low (cheap) value stocks, sell high (expensive) value stocks, buy high positive momentum stocks, and sell low negative momentum stocks. ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ea3c27-131e-4682-bb78-8dff371e7ff9",
   "metadata": {},
   "source": [
    "### DISCOUNTED CASH FLOW (DCF) VALUATION MODEL\n",
    "\n",
    "The DCF model estimates a company's intrinsic value by projecting its future cash flows and discounting them to the present value using the Weighted Average Cost of Capital (WACC). The model assumes that a company's value is the sum of its future Free Cash Flow to Firm (FCFF), adjusted for the time value of money and risk.\n",
    "\n",
    "To estimate the intrinsic value of a company, several components and financial metrics are required. FCFF represents the cash flow available to all capital providers - both equity and debt  - after accounting for operating expenses, taxes, capital expenditures, and changes in working capital. The Weighted Average Cost of Capital (WACC) is used as the discount rate in valuation, incorporating the cost of equity and cost of debt, each weighted according to the company's capital structure, to reflect its risk profile. \n",
    "\n",
    "Another critical metric is Return on Invested Capital (ROIC), which measures how efficiently a company generates returns on the capital invested in its business determining its quality. Comparing ROIC to WACC helps assess whether the firm is creating or destroying value. The intrinsic value is determined by discounted projected FCFFs and a terminal value (which captures the value beyond the explicit forecast period using a perpetual growth rate), then subtracting net debt and dividing the results by the number of shares outstanding to obtain a fair value per share. \n",
    "\n",
    "The necessary data to perform these calculations is sourced from Yahoo Finance and includes financial line items such as operating income (EBIT), taxes payable, depreciation and amortization, capital expenditures, changes in non-cash working capital, interest expense, total debt, income before tax, market capitalization, number of shares outstanding, cash balances, and beta. In addition to these, certain assumptions must be made - such as the risk-free rate, expected market return, short-term FCCF growth rate, and perpetual growth rate - as they are not directly available from financial databases. \n",
    "\n",
    "With this data, several calculations are performed: determining the cost of debt and cost of equity (using the Capital Asset Pricing Model or CAPM), computing WACC, calculating ROIC, forecasting future FCFFs, and estimating the terminal value. These inputs are then used to arrive at the fair value per share, helping us assess whether a stock is undervalued or overvalued. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d38c93-f550-44ee-9075-b683d4908530",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import textwrap\n",
    "\n",
    "def format_currency(value, prefix='R'):\n",
    "    try:\n",
    "        return f\"{prefix}{value:,.2f}\"\n",
    "    except:\n",
    "        return f\"{prefix}0.00\"\n",
    "\n",
    "def format_percentage(value):\n",
    "    try:\n",
    "        return f\"{value*100:.2f}%\"\n",
    "    except:\n",
    "        return \"0.00%\"\n",
    "        \n",
    "def calculate_dcf(ticker, growth_rate=0.07, perpetual_growth_rate=0.007, risk_free_rate=0.09, market_return=0.11, forecast_years=5):\n",
    "    \"\"\"\n",
    "    Calculate the intrinsic value per share using a DCF model. \n",
    "\n",
    "    Parameters:\n",
    "        ticker (str): Company ticker symbol \n",
    "        growth_rate (float): Annual growth rate for FCFF projections\n",
    "        perpetual_growth_rate (float): Growth rate for terminal value\n",
    "        risk_free_rate (float): Risk-free rate\n",
    "        market_return (float): Expected market return\n",
    "        forecast_years (int): Number of years for explicit forecast\n",
    "\n",
    "    Returns:\n",
    "        dict: Results including FCFF, WACC, ROIC, and fair value per share.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        company = yf.Ticker(ticker)\n",
    "        financials = company.financials.bfill(axis=1).ffill(axis=1)\n",
    "        balance_sheet = company.balance_sheet.bfill(axis=1).ffill(axis=1)\n",
    "        cashflow = company.cashflow.bfill(axis=1).ffill(axis=1)\n",
    "        info = company.info\n",
    "\n",
    "        name = info.get('shortName', 'N/A')\n",
    "        sector = info.get('sector', 'N/A')\n",
    "        industry = info.get('industry', 'N/A')\n",
    "        country = info.get('country', 'N/A')\n",
    "        website = info.get('website', 'N/A')\n",
    "        description = info.get('longBusinessSummary', 'No description available.')\n",
    "        desc_snippet = textwrap.shorten(description, width=300, placeholder=\"...\")\n",
    "\n",
    "        ebit = financials.loc['EBIT'].iloc[0] if 'EBIT' in financials.index else 0\n",
    "        interest_expense = financials.loc['Interest Expense'].iloc[0] if 'Interest Expense' in financials.index else 0\n",
    "        income_before_tax = financials.loc['Pretax Income'].iloc[0] if 'Pretax Income' in financials.index else 0\n",
    "        taxes = financials.loc['Tax Provision'].iloc[0] if 'Tax Provision' in financials.index else 0\n",
    "\n",
    "        total_debt = balance_sheet.loc['Total Debt'].iloc[0] if 'Total Debt' in balance_sheet.index else 0\n",
    "        cash_equivalents = balance_sheet.loc['Cash And Cash Equivalents'].iloc[0] if 'Cash And Cash Equivalents' in balance_sheet.index else 0\n",
    "        current_assets = balance_sheet.loc['Current Assets'].iloc[0] if 'Current Assets' in balance_sheet.index else 0\n",
    "        current_liabilities = balance_sheet.loc['Current Liabilities'].iloc[0] if 'Current Liabilities' in balance_sheet.index else 0\n",
    "        net_ppe = balance_sheet.loc['Net PPE'].iloc[0] if 'Net PPE' in balance_sheet.index else 0\n",
    "\n",
    "        depreciation = cashflow.loc['Depreciation And Amortization'].iloc[0] if 'Depreciation And Amortization' in cashflow.index else 0\n",
    "        capex = cashflow.loc['Capital Expenditure'].iloc[0] if 'Capital Expenditure' in cashflow.index else 0\n",
    "        working_capital_change = cashflow.loc['Change In Working Capital'].iloc[0] if 'Change In Working Capital' in cashflow.index else 0\n",
    "\n",
    "        beta = info.get('beta', 1.0)\n",
    "        market_cap = info.get('marketCap', 0)\n",
    "        shares_outstanding = info.get('sharesOutstanding', 1)\n",
    "        current_price = info.get('currentPrice', 0) / 100\n",
    "\n",
    "        fcff = ebit - taxes + depreciation - capex - working_capital_change\n",
    "\n",
    "        if np.isnan(fcff) or fcff <= 0:\n",
    "            net_income = financials.loc['Net Income'].iloc[0] if 'Net Income' in financials.index else 0\n",
    "            effective_tax_rate = taxes / income_before_tax if income_before_tax != 0 else 0.25\n",
    "            nopat = net_income + interest_expense * (1 - effective_tax_rate)\n",
    "            fcff = nopat + depreciation - capex - working_capital_change\n",
    "\n",
    "        effective_tax_rate = taxes / income_before_tax if income_before_tax != 0 else 0.25\n",
    "        cost_of_debt = (interest_expense / total_debt) * (1 - effective_tax_rate) if total_debt != 0 else 0\n",
    "        cost_of_equity = risk_free_rate + beta * (market_return - risk_free_rate)\n",
    "\n",
    "        total_weight = total_debt + market_cap\n",
    "        weight_debt = total_debt / total_weight if total_weight != 0 else 0\n",
    "        weight_equity = market_cap / total_weight if total_weight != 0 else 1\n",
    "\n",
    "        wacc = (weight_equity * cost_of_equity) + (weight_debt * cost_of_debt)\n",
    "        wacc = min(max(wacc, 0.05), 0.25) \n",
    "\n",
    "        invested_capital = current_assets - current_liabilities + net_ppe\n",
    "        roic = (ebit * (1 - effective_tax_rate)) / invested_capital if invested_capital != 0 else 0\n",
    "        excess_returns = roic - wacc if roic != 0 and wacc != 0 else 0\n",
    "\n",
    "        future_fcff = [fcff * (1 + growth_rate) ** t for t in range(1, forecast_years + 1)]\n",
    "        last_fcff = future_fcff[-1] if future_fcff else fcff\n",
    "        terminal_value = (last_fcff * (1 + perpetual_growth_rate)) / (wacc - perpetual_growth_rate) if wacc > perpetual_growth_rate else 0\n",
    "\n",
    "        pv_fcff = [fcff / (1 + wacc) ** t for t, fcff in enumerate(future_fcff, 1)]\n",
    "        pv_terminal = terminal_value / (1 + wacc) ** forecast_years if terminal_value != 0 else 0\n",
    "\n",
    "        total_pv = sum(pv_fcff) + pv_terminal\n",
    "        market_equity_value = total_pv + cash_equivalents - total_debt\n",
    "        fair_value_per_share = market_equity_value / shares_outstanding if shares_outstanding != 0 else 0\n",
    "\n",
    "        upside = ((fair_value_per_share - current_price) / current_price) * 100 if current_price else 0\n",
    "        margin_of_safety_pct = ((fair_value_per_share - current_price) / fair_value_per_share) * 100 if fair_value_per_share != 0 else 0\n",
    "        valuation = \"Undervalued\" if upside > 0 else \"Overvalued\"\n",
    "\n",
    "        output = f\"\\n{'='*80}\\n\"\n",
    "        output += f\"DCF Analysis for {ticker} - {name}\\n\"\n",
    "        output += f\"Sector: {sector}\\nIndustry: {industry}\\nCountry: {country}\\nWebsite: {website}\\n\"\n",
    "        output += f\"Description: {desc_snippet}\\n\\n\"\n",
    "        output += f\"FCFF: {format_currency(fcff)}\\n\"\n",
    "        output += f\"WACC: {format_percentage(wacc)}\\n\"\n",
    "        output += f\"ROIC: {format_percentage(roic)}\\n\"\n",
    "        output += f\"Excess Returns: {format_percentage(excess_returns)}\\n\"\n",
    "        output += f\"Future FCFF ({2025+1}-{2025+forecast_years}): {[format_currency(x, prefix='T') for x in future_fcff]}\\n\"\n",
    "        output += f\"PV of FCFF: {[format_currency(x) for x in pv_fcff]}\\n\"\n",
    "        output += f\"Terminal Value: {format_currency(terminal_value)}\\n\"\n",
    "        output += f\"PV of Terminal Value: {format_currency(pv_terminal)}\\n\"\n",
    "        output += f\"Market Equity Value: {format_currency(market_equity_value)}\\n\"\n",
    "        output += f\"Fair Value Per Share: {format_currency(fair_value_per_share)}\\n\"\n",
    "        output += f\"Current Price: {format_currency(current_price)}\\n\"\n",
    "        output += f\"Upside: {upside:.2f}%\\n\"\n",
    "        output += f\"Margin of Safety: {margin_of_safety_pct:.2f}%\\n\"\n",
    "        output += f\"Valuation: {valuation}\\n\"\n",
    "        output += f\"{'='*80}\\n\"\n",
    "\n",
    "        return output\n",
    "    except Exception as e:\n",
    "        return f\"Error fetching data or calculating DCF for {ticker}: {e}\"\n",
    "       \n",
    "tickers = [\"ABG.JO\", \"ADH.JO\", \"AEL.JO\", \"AFE.JO\", \"AFH.JO\", \"AFT.JO\", \"AGL.JO\", \"AHR.JO\", \"AIP.JO\", \"ANG.JO\", \"ANH.JO\", \"APN.JO\", \"ARI.JO\",\n",
    "          \"ARL.JO\", \"ATT.JO\", \"AVI.JO\", \"BAW.JO\", \"BHG.JO\", \"BID.JO\", \"BLU.JO\", \"BOX.JO\", \"BTI.JO\", \"BTN.JO\", \"BVT.JO\", \"BYI.JO\", \"CFR.JO\", \"CLS.JO\",\n",
    "          \"CML.JO\", \"COH.JO\", \"CPI.JO\", \"CSB.JO\", \"DCP.JO\", \"DRD.JO\", \"DSY.JO\", \"DTC.JO\", \"EMI.JO\", \"EQU.JO\", \"EXX.JO\", \"FBR.JO\", \"FFB.JO\", \"FSR.JO\",\n",
    "          \"FTB.JO\", \"GFI.JO\", \"GLN.JO\", \"GND.JO\", \"GRT.JO\", \"HAR.JO\", \"HCI.JO\", \"HDC.JO\", \"HMN.JO\", \"HYP.JO\", \"IMP.JO\", \"INL.JO\", \"INP.JO\", \"ITE.JO\",\n",
    "          \"JSE.JO\", \"KAP.JO\", \"KIO.JO\", \"KRO.JO\", \"KST.JO\", \"LHC.JO\", \"LTE.JO\", \"MCG.JO\", \"MKR.JO\", \"MNP.JO\", \"MRP.JO\", \"MSP.JO\", \"MTH.JO\", \"MTM.JO\",\n",
    "          \"MTN.JO\", \"N91.JO\", \"NED.JO\", \"NPH.JO\", \"NPN.JO\", \"NRP.JO\", \"NTC.JO\", \"NY1.JO\", \"OCE.JO\", \"OMN.JO\", \"OMU.JO\", \"OUT.JO\", \"PAN.JO\", \"PHP.JO\",\n",
    "          \"PIK.JO\", \"PMR.JO\", \"PPC.JO\", \"PPH.JO\", \"PRX.JO\", \"QLT.JO\", \"RBX.JO\", \"RCL.JO\", \"RDF.JO\", \"REM.JO\", \"RES.JO\", \"RLO.JO\", \"RNI.JO\", \"S32.JO\",\n",
    "          \"SAC.JO\", \"SAP.JO\", \"SBK.JO\", \"SHC.JO\", \"SHP.JO\", \"SLM.JO\", \"SNT.JO\",\"SOL.JO\", \"SPG.JO\", \"SPP.JO\", \"SRE.JO\", \"SRI.JO\", \"SSS.JO\",\"SSU.JO\",\n",
    "          \"SSW.JO\", \"SUI.JO\", \"TBS.JO\", \"TFG.JO\", \"TGA.JO\", \"TKG.JO\", \"TRU.JO\", \"TSG.JO\", \"VAL.JO\", \"VKE.JO\", \"VOD.JO\", \"WBC.JO\", \"WHL.JO\"]\n",
    "\n",
    "for ticker in tickers:\n",
    "    print(calculate_dcf(ticker))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09572a07-383a-46fa-902e-1e44d18c50f0",
   "metadata": {},
   "source": [
    "**Explanation**\n",
    "\n",
    "This value and momentum investing is an innovative investment strategy that merges the foundational principles of value and investing with the dynamics aspects of momentum investing. At its core, value investing seeks to identify stocks that are undervalued relative to their intrinsic worth. Momentum investing, on the other hand, leverages the tendency of stocks to continue moving in the same direction-upward or downward based on recent price movements. This hybrid approach aims to invest in undervalued stocks that are currently experiencing increasing demand, thereby maximizing potential returns while minimizing risk. By harnessing the strengths of both strategies, Amare Capital Management (Pty) Ltd can capitalize on market inefficiencies and identify opportunities that others may overlook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095a2b80-cce3-4fe7-a1cd-940d43e7ddc0",
   "metadata": {},
   "source": [
    "### FACTOR ALIGNED REVERSAL MODEL WITH VOLATILITY AND RISK MANAGEMENT FILTERS\n",
    "\n",
    "This strategy identifies potential bullish reversals using the hammer candlestick pattern, filtered by the asset's position relative to its above 200-day moving average to confirm the trend and volatility conditions measured by the True Range Delta. It aims to enter long positions when a hammer candle forms under specific conditions and manages risk with stop-losses, profit-targets, and special situation handling. The approach is backtested across multiple tickers to ensure robustness, reflecting Amare Capital Management's commitment to rigorous statistical validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93177d36-63f4-4c49-b445-234ed10a192a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, List \n",
    "import pandas as pd\n",
    "import os\n",
    "import yfinance as yf\n",
    "from derivative_columns.atr import add_tr_delta_col_to_ohlc\n",
    "from utils.import_data import get_local_ticker_data_file_name \n",
    "\n",
    "MUST_HAVE_DERIVATIVE_COLUMNS = {\"tr\", \"tr_delta\"}\n",
    "\n",
    "def import_yahoo_finance_daily(ticker:str) -> pd.DataFrame:\n",
    "    stock = yf.Ticker(ticker)\n",
    "    end_date = datetime.today().strftime('%Y-%m-%d')\n",
    "    df = stock.history(start=\"2020-01-01\", end= end_date, interval=\"1d\")\n",
    "    df = df[[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]]\n",
    "    df.index = pd.to.datetime(df.index).tz_localize(None)\n",
    "    return df \n",
    "\n",
    "class TickersData:\n",
    "    def __init__(self, tickers: list[str], add_features_cols_func: Callable, import_ohlc_func: Callable = import_yahoo_finance_daily):\n",
    "        self.tickers_data_with_features = {}\n",
    "        self.add_features_cols_func = add_features_cols_func\n",
    "        self.import_ohlc_func = add_features_cols_func\n",
    "        for ticker in tickers:\n",
    "            df = self.get_df_with_features(ticker=ticker)\n",
    "            for col in MUST_HAVE_DERIVATIVE_COLUMNS:\n",
    "                if col not in df.columns:\n",
    "                    df = add_tr_delta_col_to_ohlc(ohlc_df=df)\n",
    "            self.tickers_data_with_features[ticker] = df\n",
    "            \n",
    "    def get_df_with_features(self, ticker: str) -> pd.DataFrame:\n",
    "        filename_with_features = get_local_ticker_data_file_name(ticker, \"with_features\")\n",
    "        filename_raw = get_local_ticker_data_file_name(ticker, \"raw\")\n",
    "        if os.path.exists(filename_with_features):\n",
    "            return pd.read_excel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d928df5-98e7-4994-ae60-c8d61300129f",
   "metadata": {},
   "source": [
    "**Explanation**\n",
    "\n",
    "The implementation of the TickersData class enables efficient data retrieval from Yahoo Finance, with local caching in Excel files to ensure data integrity and reduce redundancy. By integrating key technical indicators such as the 200-day Moving Average (MA200), Average True Range (ATR), and hammer candle pattern detection, the firm streamlines its data management process, allowing greater focus on the development and refinement of investing strategies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978e9221-9be5-4ca9-9062-143f8e35666c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from constants2 import FEATURE_COL_NAME_ADVANCED, FEATURE_COL_NAME_BASIC \n",
    "from derivative_columns.atr import add_atr_col_to_df \n",
    "from derivative_columns.ma import add_moving_average \n",
    "from derivative_columns.hammer import add_col_is_hammer\n",
    "from derivative_columns.shooting_star import add_col_is_shooting_star\n",
    "\n",
    "MOVING_AVERAGE_N = 200\n",
    "REQUIRED_DERIVATIVE_COLUMNS_F_V1_BASIC = {\"atr_14\", f\"ma_{MOVING_AVERAGE_N}\", \"is_hammer\", \"is_shooting_star\"}\n",
    "\n",
    "def add_required_cols_for_f_v1_basic(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_columns = df.columns \n",
    "    internal_df = df.copy()\n",
    "    if f\"ma_{MOVING_AVERAGE_N}\" not in df_columns:\n",
    "        internal_df = add_moving_average(df=internal_df, n=MOVING_AVERAGE_N)\n",
    "    if \"atr_14\" not in df_columns:\n",
    "        internal_df = add_atr_col_to_df(df=internal_df, n=14, exponential=False)\n",
    "    if \"is_hammer\" not in df_columns:\n",
    "        internal_df = add_col_is_hammer(df=internal_df)\n",
    "    if \"is_shooting_star\" not in df_columns:\n",
    "        internal_df = add_col_is_shooting_star(df=internal_df)\n",
    "    return internal_df \n",
    "\n",
    "def add_features_v1_basic(df: pd.DataFrame, atr_multiplier_threshold: int = 6) -> pd.DataFrame:\n",
    "    res = df.copy()\n",
    "    for col in REQUIRED_DERIVATIVE_COLUMNS_F_V1_BASIC:\n",
    "        if col not in res.columns:\n",
    "            res = add_required_cols_for_f_v1_basic(df=res)\n",
    "    res[FEATURE_COL_NAME_BASIC] = res[\"Close\"] < res[f\"ma_{MOVING_AVERAGE_N}\"]\n",
    "    res[FEATURE_COL_NAME_ADVANCED] = (res[\"ma_200\"] - res[\"Close\"]) >= (res[\"atr_14\"] * atr_multiplier_threshold)\n",
    "    return res                                                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0155bb3f-9961-4538-af32-02cdcc1a4330",
   "metadata": {},
   "source": [
    "**Explanation**\n",
    "\n",
    "The add_features_v1_basic function is enhanced to incorporate a hammer candle signal (is_hammer) and to refine FEATURE_COL_NAME_ADVANCED to activate when the stock price is significantly below the 200-day Moving Average (MA200) with a confirmed hammer pattern. This transformation of complex market data into clear, actionable signals supports the firm's mission to elevate investing decisions through statistically grounded methodologies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbea922-024d-4b74-88ec-67a24e3d7566",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "from backtesting.backtesting import Strategy \n",
    "from backtesting import set_bokeh_output\n",
    "set_bokeh_output(notebook=False)\n",
    "from constants2 import DPS_STUB, FEATURE_COL_NAME_ADVANCED\n",
    "from utils.strategy_exec.misc import get_current_position_size \n",
    "\n",
    "def get_desired_current_position_size(strategy: Strategy) -> Tuple[Optional[float], float, str]:\n",
    "    current_position_size = (\n",
    "        get_current_position_size(\n",
    "            shares_count=strategy.position.size,\n",
    "            equity=strategy.equity,\n",
    "            last_price=strategy._data.Open[-1],\n",
    "        )\n",
    "        if strategy.position.size != 0 \n",
    "        else 0\n",
    "    )\n",
    "    is_hammer = strategy._data[\"is_hammer\"][-1]\n",
    "    price_below_ma200 = strategy._data[FEATURE_COL_NAME_ADVANCED][-1]\n",
    "    volatility_ok = strategy.data[\"tr_delta\"][-1] < 2.5 \n",
    "\n",
    "    desired_position_size: Optional[float] = None \n",
    "    message = DPS_STUB\n",
    "\n",
    "    if current_position_size != 0:\n",
    "        desired_position_size = current_position_size\n",
    "        message = \"Maintain existing position\"\n",
    "        return desired_position_size, current_position_size, message\n",
    "\n",
    "    if is_hammer and price_below_ma200 and volatility_ok:\n",
    "        desired_position_size = 1.0\n",
    "        message = \"Enter Long: Hammer reversal below MA200 with moderate volatility\"\n",
    "\n",
    "    return desired_position_size, current_position_size, message "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425910ec-376e-4602-bf22-a342b587d6ee",
   "metadata": {},
   "source": [
    "**Explanation**\n",
    "\n",
    "The get_desired_current_position_size function is modified to initiate a 100% when a hammer candle forms, the price is at least six times the ATR_14 below the 200-day Moving Average (MA200), and volatility (True Range Delta) remains moderate (below 2.5). The function also represents existing entries or exits under specific conditions, reinforcing the firm's disciplined capital allocation framework - an essential pillar of its systematic and data-driven investment strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6581110d-8cee-4040-9dfd-4f17e7744ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "from backtesting import Strategy \n",
    "from constants2 import SL_TIGHTENED \n",
    "import numpy as np\n",
    "\n",
    "def _get_n_atr(strategy: Strategy) -> float:\n",
    "    index = len(strategy.data) - 1\n",
    "    if strategy.data.tr_delta[index] > 1.98 and strategy.trades and strategy.trades[-1].pl > 0:\n",
    "        return 1.1 \n",
    "    return strategy_parameters.stop_loss_default_atr_multiplier\n",
    "\n",
    "def update_stop_loss(strategy: Strategy):\n",
    "    if not strategy.trades:\n",
    "        return \n",
    "    n_atr = _get_n_atr(strategy)\n",
    "    index = len(strategy.data) - 1\n",
    "    for trade in strategy.trades:\n",
    "        if trade.is_long:\n",
    "            sl_price = max(trade.sl or -np.inf, strategy.data.Open[index] - strategy.data.atr_14[index] * n_atr)\n",
    "        else:\n",
    "            sl_price = min(trade.sl or np.inf, strategy.data.Open[index] + strategy.data.atr_14[index] * n_atr)\n",
    "        if sl_price < 0:\n",
    "            sl_price = None \n",
    "        if sl_price and trade.sl != sl-price: \n",
    "            trade.sl = sl-price \n",
    "            if n_atr == 1.1 and SL_TIGHTENED not in (trade.tag or \"\"):\n",
    "                setattr(trade, f\"_{trade.__class__.__qualname__}__tag\", (trade.tag or \"\") + SL_TIGHTENED)\n",
    "\n",
    "def check_set_profit_targets_long_trades(strategy: Strategy):\n",
    "    last_price = strategy._data.Open[-1]\n",
    "    min_profit_target_long = None \n",
    "    trades_long = [trade for trade in strategy.trades if trade.is_long]\n",
    "    for trade in trades_long:\n",
    "        if trade.tp is not None:\n",
    "            min_profit_target_long = min(min_profit_target_long or trade.tp, trade.tp)\n",
    "        if trades_long and min_profit_target_long  is None:\n",
    "            min_profit_target_long = (float(strategy.parameters.profit_target_long_pct + 100) / 100) * last_price\n",
    "            for trade in trades_long:\n",
    "                if trade.tp is None:\n",
    "                    trade.tp = min_profit_target_long"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0fd096-b8f3-43c6-859a-3d0fb3fcff9a",
   "metadata": {},
   "source": [
    "**Explanation**\n",
    "\n",
    "The existing function such as update_stop_losses and check_set_profit_targets_long_trades are utilized with default parameters - a 2.5 ATR multiplier for stop-losses and a 29.9% profit target - to manage trades effectively. During periods of elevated volatility (when true range delta exceeds 1.98), stop-losses are tightened to 1.1 ATR, reinforcing the firm's commitment to robust risk management practice that underpin its pursuit of consistent and sustainable performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faacff19-3114-4567-bf46-1c9eff189415",
   "metadata": {},
   "outputs": [],
   "source": [
    "from backtesting import Strategy \n",
    "from constants2 import CLOSED_VOLATILITY_SPIKE, CLOSED_MAX_DURATION, SS_VOLATILITY_SPIKE, SS_MAX_DURATION, SS_NO_TODAY\n",
    "from utils.strategy_exec.misc import add_tag_to_trades_and_close_position\n",
    "\n",
    "def process_volatility_spike(strategy: Strategy) -> bool:\n",
    "    if strategy.data.tr_delta[-1] < 2.5:\n",
    "        return False \n",
    "    add_tag_to_trades_and_close_position(strategy, CLOSED_VOLATILITY_SPIKE)\n",
    "    return True \n",
    "\n",
    "def process_max_duration(strategy: Strategy) -> bool:\n",
    "    max_trade_duration_long = strategy.parameters.max_trade_duration_long\n",
    "    if max_trade_duration_long is None or not strategy.trades:\n",
    "        return False\n",
    "    max_trade_duration = max((strategy.data.index[-1] - trade.entry_time).days for trade in strategy.trades)\n",
    "    if strategy.trades[-1].is_long and max_trade_duration > max_trade_duration_long:\n",
    "        add_tag_to_trades_and_close_position(strategy, CLOSED_MAX_DURATION)\n",
    "        return True\n",
    "    return False \n",
    "\n",
    "def process_special_situations(strategy: Strategy) -> Tuple[bool, str]: \n",
    "    if process_max_duration(strategy):\n",
    "        return True, SS_MAX_DURATION \n",
    "    if process_volatility_spike(strategy):\n",
    "        return True, SS_VOLATILITY_SPIKE\n",
    "    return False, SS_NO_TODAY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0422e7-e249-44ee-a1b5-2f84d5d96c43",
   "metadata": {},
   "source": [
    "**Explanation**\n",
    "\n",
    "The process_special_situation function is employed to automatically close positions during extreme volatility spikes  (true range delta > 2.5) or when positions exceed a maximum holding period of 100 days. This proactive approach strengthens the resilience of the investing strategy, aligning with the firm's long-term vision of maintaining robust performance across varying market conditions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05cdd3c-60da-4872-9c9f-0526a5c4f5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from customizable.strategy_params import StrategyParams\n",
    "from utils.local_data import TickersData\n",
    "from strategy.run_backtest_for_ticker import run_backtest_for_ticker\n",
    "\n",
    "def run_all_tickers(tickers_data: TickersData, strategy_params: StrategyParams, tickers: list[str]) -> float:\n",
    "    open(\"app_run.log\", \"w\", encoding=\"UTF-8\").close()\n",
    "    performance_res = pd.Dataframe()\n",
    "    all_trades = pd.DataFrame()\n",
    "    for ticker in tickers:\n",
    "        ticker_data = tickers_data.get_data(ticker)\n",
    "        stat, trades_df, last_day_result = run_backtest_for_ticker(ticker, ticker_data, strategy_params)\n",
    "        stat = stat.drop([\"_strategy\", \"_equity_curve\", \"_trades\"])\n",
    "        stat[\"SQN_MODIFIED\"] =stat[\"SQN\"] / np.sqrt(stat[\"# Trades\"])\n",
    "        performance_res[ticker] = stat\n",
    "        if strategy_params.save_all_trades_in_xlsx:\n",
    "            trades_df[\"Ticker\"] = ticker\n",
    "            all_trades = pd.concat([all_trades, trades_df])\n",
    "    if len(tickers) > 1:\n",
    "        performance_res.to_excel(\"output.xlsx\")\n",
    "    if strategy_params.save_all_trades_in_xlsx:\n",
    "        all_trades.to_excel(\"all_trades.xlsx\", index=False)\n",
    "    return performance_res.loc[\"SQN_modified\", :].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757089d6-b4f0-49e0-8705-17a55c5ce972",
   "metadata": {},
   "source": [
    "**Explanation**\n",
    "\n",
    "Comprehensive backtests are conducted across all tickers in tickers_all using the run_all_tickers function, while key parameters _ such as atr_multiplier_threshold are fine tuned through run_strategy_main_optimize. This rigorous testing process validates the strategy's effectiveness and reflects the firm's commitment to a data-driven, evidence-based investment approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec799621-fc53-43cd-90f4-d0e9b764ccfc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import logging \n",
    "from dotenv import load_dotenv \n",
    "from constants2 import LOG_FILE, tickers_all\n",
    "from customizable.strategy_params import StrategyParams \n",
    "from f_v1_basic import add_features_v1_basic \n",
    "from strategy.all_tickers import run_all_tickers \n",
    "from utils.local_data import TickersData\n",
    "import warnings \n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG, format=\"%(message)s\", filename=LOG_FILE, encoding=\"utf-8\", filemode=\"a\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    load_dotenv()\n",
    "    open(LOG_FILE, \"w\", encoding=\"UTF-8\").close()\n",
    "\n",
    "    strategy_params = StrategyParams(\n",
    "        max_trade_duration_long=100,\n",
    "        max_trade_duration_short=100,\n",
    "        profit_target_long_pct=29.9,\n",
    "        profit_target_short_pct=29.9,\n",
    "        stop_loss_default_atr_multiplier=2.5,\n",
    "        save_all_trades_in_xlsx= True,\n",
    "    )\n",
    "\n",
    "    tickers_data = TickersData (\n",
    "        add_feature_cols_func=add_features_v1_basic,\n",
    "        tickers=tickers_all,\n",
    "    )\n",
    "\n",
    "    SQN_modified_mean = run_all_tickers(\n",
    "        tickers_data=tickers_data,\n",
    "        tickers=tickers_all,\n",
    "        strategy_params=strategy_params,\n",
    "    )\n",
    "    logging.debug(f\"SQN_modified_mean={SQN_modified_mean}\")\n",
    "    print(f\"SQN_modified_mean={SQN_modified_mean}, see output.xlsx\") \n",
    "    \n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b83014f-8646-4af8-b4b9-44a028bd2b82",
   "metadata": {},
   "source": [
    "**Explanation**\n",
    "\n",
    "The last_day_result function is used to monitor investing signals for real-time execution, with outcomes saved in output.xlsx for ongoing analysis. This continuos monitoring framework supports the firm's commitment to iterative strategy refinement and long-term performance improvement.\n",
    "\n",
    "The System Quality Number (SQN) is a popular indicator of the investing system's quality. Its classic formula has a drawback: it tends to produce overly optimistic results when analyzing more than 100 orders, particularly when the number of orders exceeds 150-200.\n",
    "\n",
    "SQN_modified is devoid of this drawback. It is simply the average of profits divided by the standard deviation of profits. An investing system is considered not bad if its SQN_modified has a positive value of at least 0.1. Systems whose value exceeds 0.2 are deemed decent or even good. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49775c78-04c4-4585-b184-92a2677424b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from typing import List\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "from contextlib import redirect_stdout\n",
    "import os\n",
    "\n",
    "from f_v1_basic import add_features_v1_basic\n",
    "from derivative_columns.shooting_star import add_col_is_shooting_star\n",
    "from derivative_columns.atr import add_tr_delta_col_to_ohlc\n",
    "\n",
    "LOG_FILE = \"app_run.log\"\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=\"%(message)s\",\n",
    "    filename=LOG_FILE,\n",
    "    encoding=\"utf-8\",\n",
    "    filemode=\"a\",\n",
    ")\n",
    "\n",
    "def fetch_ohlc_yfinance(ticker: str, start_date: str = \"2020-01-01\", end_date: str = None) -> pd.DataFrame:\n",
    "    if end_date is None:\n",
    "        end_date = end_date = datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "    try:\n",
    "        df = yf.Ticker(ticker).history(start=start_date, end=end_date, interval=\"1d\")\n",
    "        if df.empty:\n",
    "            logging.error(f\"No data fetched for {ticker}\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        df = df[[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]]\n",
    "        df.index = pd.to_datetime(df.index).tz_localize(None)  # Remove timezone\n",
    "        logging.debug(f\"Fetched {len(df)} rows for {ticker}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error fetching data for {ticker}: {str(e)}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def generate_bearish_signals(tickers: List[str], start_date: str = \"2020-01-01\", end_date: str = None) -> pd.DataFrame:\n",
    "    if end_date is None:\n",
    "        end_date = datetime.today().strftime('%Y-%m-%d')\n",
    "   \n",
    "    results = []\n",
    "    \n",
    "    with open(os.devnull, 'w') as devnull:\n",
    "        with redirect_stdout(devnull):\n",
    "            for ticker in tickers:\n",
    "                logging.debug(f\"Processing ticker: {ticker}\")\n",
    "           \n",
    "                df = fetch_ohlc_yfinance(ticker, start_date, end_date)\n",
    "                if df.empty:\n",
    "                    logging.warning(f\"Skipping {ticker} due to empty data\")\n",
    "                    continue\n",
    "                \n",
    "                df = add_features_v1_basic(df)\n",
    "                \n",
    "                df = add_col_is_shooting_star(df)\n",
    "                df = add_tr_delta_col_to_ohlc(df)\n",
    "                \n",
    "                logging.debug(f\"Data shape for {ticker}: {df.shape}\")\n",
    "                logging.debug(f\"Columns for {ticker}: {list(df.columns)}\")\n",
    "                \n",
    "                nan_counts = df[['Close', 'ma_200', 'atr_14', 'tr_delta', 'is_shooting_star']].isna().sum()\n",
    "                logging.debug(f\"NaN counts for {ticker}:\\n{nan_counts}\")\n",
    "                \n",
    "                df[\"Bearish_Signal\"] = (\n",
    "                    (df[\"is_shooting_star\"] == True) &  \n",
    "                    (df[\"Close\"] > df[\"ma_200\"]) &    \n",
    "                    (df[\"tr_delta\"] < 3.0)             \n",
    "                )\n",
    "                \n",
    "                shooting_star_count = df[\"is_shooting_star\"].sum()\n",
    "                uptrend_count = (df[\"Close\"] > df[\"ma_200\"]).sum()\n",
    "                volatility_count = (df[\"tr_delta\"] < 3.0).sum()\n",
    "                signal_count = df[\"Bearish_Signal\"].sum()\n",
    "                logging.debug(f\"Shooting star count for {ticker}: {shooting_star_count}\")\n",
    "                logging.debug(f\"Uptrend count (Close > ma_200) for {ticker}: {uptrend_count}\")\n",
    "                logging.debug(f\"Volatility count (tr_delta < 3.0) for {ticker}: {volatility_count}\")\n",
    "                logging.debug(f\"Bearish signal count for {ticker}: {signal_count}\")\n",
    "                \n",
    "                df.to_excel(f\"debug_{ticker}_full_data.xlsx\")\n",
    "                logging.debug(f\"Saved full data for {ticker} to debug_{ticker}_full_data.xlsx\")\n",
    "                \n",
    "                df_output = df[[\"Close\", \"ma_200\", \"atr_14\", \"tr_delta\", \"is_shooting_star\", \"Bearish_Signal\"]].copy()\n",
    "                df_output[\"Ticker\"] = ticker\n",
    "                df_output[\"Date\"] = df_output.index\n",
    "                df_output[\"Distance_to_MA200\"] = ((df[\"Close\"] - df[\"ma_200\"]) / df[\"atr_14\"]).round(2)\n",
    "                df_output = df_output[[\"Ticker\", \"Date\", \"Close\", \"ma_200\", \"atr_14\", \"tr_delta\", \"Distance_to_MA200\", \"is_shooting_star\", \"Bearish_Signal\"]]\n",
    "                \n",
    "                results.append(df_output[df_output[\"Bearish_Signal\"] == True])\n",
    "    \n",
    "    result_df = pd.concat(results) if results else pd.DataFrame(\n",
    "        columns=[\"Ticker\", \"Date\", \"Close\", \"ma_200\", \"atr_14\", \"tr_delta\", \"Distance_to_MA200\", \"is_shooting_star\", \"Bearish_Signal\"]\n",
    "    )\n",
    "    \n",
    "    result_df[[\"Close\", \"ma_200\", \"atr_14\", \"tr_delta\"]] = result_df[[\"Close\", \"ma_200\", \"atr_14\", \"tr_delta\"]].round(2)\n",
    "\n",
    "    output_file = \"bearish_signals.xlsx\"\n",
    "    result_df.to_excel(output_file, index=False)\n",
    "    logging.debug(f\"Bearish signals saved to {output_file}\")\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    load_dotenv()\n",
    "    \n",
    "    open(LOG_FILE, \"w\", encoding=\"utf-8\").close()\n",
    "    \n",
    "    custom_tickers = [\"ABG.JO\", \"ADH.JO\", \"AEL.JO\", \"AFE.JO\", \"AFH.JO\", \"AFT.JO\", \"AGL.JO\", \"AHR.JO\", \"AIP.JO\", \"ANG.JO\", \"ANH.JO\", \"APN.JO\", \"ARI.JO\",\n",
    "          \"ARL.JO\", \"ATT.JO\", \"AVI.JO\", \"BAW.JO\", \"BHG.JO\", \"BID.JO\", \"BLU.JO\", \"BOX.JO\", \"BTI.JO\", \"BTN.JO\", \"BVT.JO\", \"BYI.JO\", \"CFR.JO\", \"CLS.JO\",\n",
    "          \"CML.JO\", \"COH.JO\", \"CPI.JO\", \"CSB.JO\", \"DCP.JO\", \"DRD.JO\", \"DSY.JO\", \"DTC.JO\", \"EMI.JO\", \"EQU.JO\", \"EXX.JO\", \"FBR.JO\", \"FFB.JO\", \"FSR.JO\",\n",
    "          \"FTB.JO\", \"GFI.JO\", \"GLN.JO\", \"GND.JO\", \"GRT.JO\", \"HAR.JO\", \"HCI.JO\", \"HDC.JO\", \"HMN.JO\", \"HYP.JO\", \"IMP.JO\", \"INL.JO\", \"INP.JO\", \"ITE.JO\",\n",
    "          \"JSE.JO\", \"KAP.JO\", \"KIO.JO\", \"KRO.JO\", \"KST.JO\", \"LHC.JO\", \"LTE.JO\", \"MCG.JO\", \"MKR.JO\", \"MNP.JO\", \"MRP.JO\", \"MSP.JO\", \"MTH.JO\", \"MTM.JO\",\n",
    "          \"MTN.JO\", \"N91.JO\", \"NED.JO\", \"NPH.JO\", \"NPN.JO\", \"NRP.JO\", \"NTC.JO\", \"NY1.JO\", \"OCE.JO\", \"OMN.JO\", \"OMU.JO\", \"OUT.JO\", \"PAN.JO\", \"PHP.JO\",\n",
    "          \"PIK.JO\", \"PMR.JO\", \"PPC.JO\", \"PPH.JO\", \"PRX.JO\", \"QLT.JO\", \"RBX.JO\", \"RCL.JO\", \"RDF.JO\", \"REM.JO\", \"RES.JO\", \"RLO.JO\", \"RNI.JO\", \"S32.JO\",\n",
    "          \"SAC.JO\", \"SAP.JO\", \"SBK.JO\", \"SHC.JO\", \"SHP.JO\", \"SLM.JO\", \"SNT.JO\", \"SOL.JO\", \"SPG.JO\", \"SPP.JO\", \"SRE.JO\", \"SRI.JO\", \"SSS.JO\",\n",
    "          \"SSU.JO\", \"SSW.JO\", \"SUI.JO\", \"TBS.JO\", \"TFG.JO\", \"TGA.JO\", \"TKG.JO\", \"TRU.JO\", \"TSG.JO\", \"VAL.JO\", \"VKE.JO\", \"VOD.JO\", \"WBC.JO\", \"WHL.JO\"]\n",
    "    \n",
    "    bearish_signals_df = generate_bearish_signals(tickers=custom_tickers)\n",
    "\n",
    "    pd.set_option('display.width', 1000)\n",
    "    pd.set_option('display.max_columns', None)\n",
    "\n",
    "    print(f\"Generated bearish signals for {len(custom_tickers)} tickers.\")\n",
    "    print(f\"Total signals: {len(bearish_signals_df)}\")\n",
    "    print(f\"Results saved to bearish_signals.xlsx\")\n",
    "    if not bearish_signals_df.empty:\n",
    "        print(\"\\nSample of bearish signals:\")\n",
    "        print(bearish_signals_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d80c98d-fc25-4f8f-ba81-c3ab70cc796e",
   "metadata": {},
   "source": [
    "**Explanation** \n",
    "\n",
    "The objective of this step is to generate bearish signals by detecting potential reversal points using the shooting star candlestick pattern in an uptrend, filtered by momentum and volatility conditions. The shooting star pattern is identified when is_shooting_star == True, with an uptrend filter where the price is above the 200-day moving average (Close > ma_200). The volatility filter ensures that the True Range delta (tr_delta) is below 2.5 for moderate volatility. Additionally, an optional condition checks if the price is significantly above the 200-day MA ((Close - ma_200) >= (atr_14 * 6)). The output is saved in a DataFrame (bearish_signals.xlsx) with columns for Ticker, Date, Close, MA200, ATR_14, TR_Delta, Is_Shooting_Star, and Bearish_Signal, helping investors identify stocks to avoid buying at potential peaks or consider for short positions. This approach focuses on real-time monitoring without backtesting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e71c33-21cb-4b68-bee4-b92c83bd4f1d",
   "metadata": {},
   "source": [
    "### ANCHORED VOLUME WEIGHTED AVERAGE PRICE (AVWAPs)\n",
    "\n",
    "\n",
    "This systematic investing strategy utilizes Anchored VWAPs to identify momentum in trends, support and resistance levels, and optimal entry and exit points. Primarily designed for daily (1d) low-frequency investing, it can also be adapted for intraday timeframes like 15-minute or 5-minute charts for mid-frequency investing. The strategy focuses on stocks, incorporating key indicators such as Anchored VWAPs, the Average True Range (ATR), significant price levels, and a 5-day Simple Moving Average (SMA). Anchored VWAP, a powerful tool in technical analysis, calculates the volume-weighted average price of an asset from a specific anchor point, such as key highs, lows, or market events, offering dynamic support and resistance levels. It is computed as the cumulative sum of the Typical Price (Open + High + Low + Close)/4 multiplied by volume, divided by total volume. This momentum approach helps confirm price trends, identify support and resistance zones, and generate investing signals based on price interactions with VWAP levels. The provided code framework efficiently fetches OHLC data, computes Anchored VWAPs, detects significant price levels, and visualizes them, ensuring a systematic and repeatable investing process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a93c0f-7c74-4566-9d57-f6e04ed3570a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from import_ohlc.yahoo_finance import get_ohlc_from_yf\n",
    "from misc.atr import add_atr_col_to_df\n",
    "from misc.fill_min_max import fill_is_min_max\n",
    "from constants import ATR_SMOOTHING_N\n",
    "\n",
    "def prepare_data(ticker: str, period: str= \"2y\", interval: str = \"1d\") -> pd.DataFrame:\n",
    "    try:\n",
    "        df = get_ohlc_from_yf(ticker=ticker, period=period, interval=interval)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch data for {ticker} from Yahoo Finance: {e}\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    df = add_atr_col_to_df(df, n=ATR_SMOOTHING_N, exponential=False)\n",
    "\n",
    "    df = fill_is_min_max(df)\n",
    "\n",
    "    print(f\"Prepared data for {ticker}:\")\n",
    "    print(df[[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\", f\"atr_{ATR_SMOOTHING_N}\", \"is_min\", \"is_max\"]].tail())\n",
    "    return df\n",
    "    \n",
    "ticker_data = {}\n",
    "tickers = [\"ABG.JO\", \"ADH.JO\", \"AEL.JO\", \"AFE.JO\", \"AFH.JO\", \"AFT.JO\", \"AGL.JO\", \"AHR.JO\", \"AIP.JO\", \"ANG.JO\", \"ANH.JO\", \"APN.JO\", \"ARI.JO\",\n",
    "          \"ARL.JO\", \"ATT.JO\", \"AVI.JO\", \"BAW.JO\", \"BHG.JO\", \"BID.JO\", \"BLU.JO\", \"BOX.JO\", \"BTI.JO\", \"BTN.JO\", \"BVT.JO\", \"BYI.JO\", \"CFR.JO\", \"CLS.JO\",\n",
    "          \"CML.JO\", \"COH.JO\", \"CPI.JO\", \"CSB.JO\", \"DCP.JO\", \"DRD.JO\", \"DSY.JO\", \"DTC.JO\", \"EMI.JO\", \"EQU.JO\", \"EXX.JO\", \"FBR.JO\", \"FFB.JO\", \"FSR.JO\",\n",
    "          \"FTB.JO\", \"GFI.JO\", \"GLN.JO\", \"GND.JO\", \"GRT.JO\", \"HAR.JO\", \"HCI.JO\", \"HDC.JO\", \"HMN.JO\", \"HYP.JO\", \"IMP.JO\", \"INL.JO\", \"INP.JO\", \"ITE.JO\",\n",
    "          \"JSE.JO\", \"KAP.JO\", \"KIO.JO\", \"KRO.JO\", \"KST.JO\", \"LHC.JO\", \"LTE.JO\", \"MCG.JO\", \"MKR.JO\", \"MNP.JO\", \"MRP.JO\", \"MSP.JO\", \"MTH.JO\", \"MTM.JO\",\n",
    "          \"MTN.JO\", \"N91.JO\", \"NED.JO\", \"NPH.JO\", \"NPN.JO\", \"NRP.JO\", \"NTC.JO\", \"NY1.JO\", \"OCE.JO\", \"OMN.JO\", \"OMU.JO\", \"OUT.JO\", \"PAN.JO\", \"PHP.JO\",\n",
    "          \"PIK.JO\", \"PMR.JO\", \"PPC.JO\", \"PPH.JO\", \"PRX.JO\", \"QLT.JO\", \"RBX.JO\", \"RCL.JO\", \"RDF.JO\", \"REM.JO\", \"RES.JO\", \"RLO.JO\", \"RNI.JO\", \"S32.JO\",\n",
    "          \"SAC.JO\", \"SAP.JO\", \"SBK.JO\", \"SHC.JO\", \"SHP.JO\", \"SLM.JO\", \"SNT.JO\", \"SOL.JO\", \"SPG.JO\", \"SPP.JO\", \"SRE.JO\", \"SRI.JO\", \"SSS.JO\",\n",
    "          \"SSU.JO\", \"SSW.JO\", \"SUI.JO\", \"TBS.JO\", \"TFG.JO\", \"TGA.JO\", \"TKG.JO\", \"TRU.JO\", \"TSG.JO\", \"VAL.JO\", \"VKE.JO\", \"VOD.JO\", \"WBC.JO\", \"WHL.JO\"]\n",
    "\n",
    "for ticker in tickers: \n",
    "    df = prepare_data(ticker)\n",
    "    if not df.empty:\n",
    "        ticker_data[ticker] = df "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d568d7-cae0-4856-abb0-13d3bd738a52",
   "metadata": {},
   "source": [
    "**Explanation**\n",
    "\n",
    "The code fetches two years of daily OHLCV data for the ticker using the get_ohlc_from_yf function, computes a 14-period ATR (shifted to the previous day's value), and identifies significant highs and lows where price movements exceed 2.5 times the ATR. The resulting data, including OHLCV, ATR, and marked min/max levels, is stored in a dictionary of DataFrames for further analysis. Key functions from the codebase, such as add_atr_col_to_df for calculating volatility and fill_is_min_max for detecting significant levels, are used. This data provides the necessary foundation for VWAP-based investing and risk management. The output is a dictionary of DataFrames, printed for verification, containing the OHLCV, ATR, and key level indicators.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5d1ef0-30a3-46c1-aca8-712b4dfa0165",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from constants import first_day_of_year\n",
    "\n",
    "def get_anchor_dates(df: pd.DataFrame, custom_dates: list[str] = None) -> list[str]:\n",
    "    last_min_date = df[df[\"is_min\"]].index.max()\n",
    "    last_max_date = df[df[\"is_max\"]].index.max()\n",
    "\n",
    "    anchor_dates = [first_day_of_year]\n",
    "    \n",
    "    if pd.notna(last_min_date):\n",
    "        anchor_dates.append(last_min_date.strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    if pd.notna(last_max_date):\n",
    "        anchor_dates.append(last_max_date.strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "    anchor_dates = [date for date in anchor_dates if pd.notna (date)]\n",
    "    print(f\"Anchor dates for {df.attrs.get('ticker', 'unknown')}: {anchor_dates}\")\n",
    "    return anchor_dates\n",
    "\n",
    "anchor_dates_dict = {\n",
    "    ticker: get_anchor_dates(df.assign(attrs={\"ticker\": ticker}))\n",
    "    for ticker, df in ticker_data.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfd9082-d657-4a5f-bc57-c5c648be1059",
   "metadata": {},
   "source": [
    "**Explanation**\n",
    "\n",
    "The code creates a list of anchor dates for a specific stock ticker, starting with the fixed baseline date, first_day_of_year. It then adds custom_dates (such as earning reports, price peaks, and market corrections), and incorporates the most recent min/max dates from the stock's price history defined in prepare data function. The anchored dates are stored in a dictionary, providing a set of key reference points for further analysis, like VWAP calculations or investing signals. Custom dates are chosen to reflect significant events, such as earning reports or major price movements, which help us investors make data-driven decisions. The code automates the process of adding these anchor points, ensuring they align with key market events, while offering flexibility for our strategies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4146d24-2c39-44fc-acdd-e5c49897451e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from vwaps_plot import vwaps_plot_build_save\n",
    "from misc.chart_annotation import get_chart_annotation_1d \n",
    "from constants import ATR_SMOOTHING_N, first_day_of_year\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def analyze_ticker(df: pd.DataFrame, ticker: str, anchor_dates: list[str]):\n",
    "    df.attrs[\"ticker\"] = ticker\n",
    "    df = df.copy()\n",
    "    \n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    if df.index.tz is not None:\n",
    "        df.index = df.index.tz_convert(None)\n",
    "    \n",
    "    dates_only = df.index.normalize().date\n",
    "\n",
    "    for i, anchor_date in enumerate(anchor_dates, 1):\n",
    "        anchor_ts = pd.Timestamp(anchor_date).date()\n",
    "        if anchor_ts in dates_only:\n",
    "            anchor_idx = list(dates_only).index(anchor_ts)\n",
    "        else:\n",
    "            if anchor_ts < df.index[0].date():\n",
    "                print(f\"Info: Anchor date {anchor_date} is before the start of data for {ticker}. Using first available date.\")\n",
    "            else:\n",
    "                #print(f\"Warning: Anchor date {anchor_date} not found in {ticker} index. Using start of data.\")\n",
    "                anchor_idx = 0\n",
    "\n",
    "        df_from_anchor = df.iloc[anchor_idx:]\n",
    "        typical_price = (df_from_anchor[\"Open\"] + df_from_anchor[\"High\"] +\n",
    "                         df_from_anchor[\"Low\"] + df_from_anchor[\"Close\"]) / 4\n",
    "        cumulative_typical_volume = (typical_price * df_from_anchor[\"Volume\"]).cumsum()\n",
    "        cumulative_volume = df_from_anchor[\"Volume\"].cumsum()\n",
    "        vwap = cumulative_typical_volume / cumulative_volume.replace(0, pd.NA)\n",
    "        vwap = vwap.ffill()\n",
    "        df.loc[df_from_anchor.index, f\"A_VWAP_{i}\"] = vwap\n",
    "\n",
    "    vwaps_plot_build_save(\n",
    "        input_df=df,\n",
    "        anchor_dates=anchor_dates,\n",
    "        chart_title=f\"{ticker} Daily with Anchored VWAPs\",\n",
    "        chart_annotation_func=get_chart_annotation_1d,\n",
    "        add_last_min_max=False,\n",
    "        file_name=f\"daily_{ticker}_annotated.png\",\n",
    "        print_df=False  \n",
    "    )\n",
    "   \n",
    "    last_close = df[\"Close\"].iloc[-1]\n",
    "    vwap_year = df[\"A_VWAP_1\"].iloc[-1]\n",
    "    vwap_min = df[\"A_VWAP_2\"].iloc[-1]\n",
    "    vwap_max = df[\"A_VWAP_3\"].iloc[-1]\n",
    "    atr = df[f\"atr_{ATR_SMOOTHING_N}\"].iloc[-1]\n",
    "\n",
    "    trend = \"Neutral\"\n",
    "    if last_close > vwap_year and last_close > vwap_min:\n",
    "        trend = \"Bullish\"\n",
    "    elif last_close < vwap_year and last_close < vwap_max:\n",
    "        trend = \"Bearish\"\n",
    "\n",
    "    signal = None\n",
    "    if trend == \"Bullish\" and last_close > vwap_min and abs(last_close - vwap_min) < atr * 0.5:\n",
    "        signal = \"Long\"\n",
    "    elif trend == \"Bearish\" and last_close < vwap_max and abs(vwap_max - last_close) < atr * 0.5:\n",
    "        signal = \"Short\"\n",
    "\n",
    "    if signal == \"Long\":\n",
    "        entry_price = last_close\n",
    "        stop_loss = vwap_min - atr\n",
    "        take_profit = vwap_max\n",
    "        risk = entry_price - stop_loss\n",
    "    elif signal == \"Short\":\n",
    "        entry_price = last_close\n",
    "        stop_loss = vwap_max + atr\n",
    "        take_profit = vwap_min\n",
    "        risk = stop_loss - entry_price\n",
    "    else:\n",
    "        entry_price = stop_loss = take_profit = risk = None\n",
    "\n",
    "    account_size = 10000\n",
    "    risk_percent = 0.01\n",
    "    position_size = (account_size * risk_percent) / risk if risk else 0\n",
    "\n",
    "    return {\n",
    "        \"trend\": trend,\n",
    "        \"signal\": signal,\n",
    "        \"last_close\": last_close,\n",
    "        \"vwap_year\": vwap_year,\n",
    "        \"vwap_min\": vwap_min,\n",
    "        \"vwap_max\": vwap_max,\n",
    "        \"atr\": atr,\n",
    "        \"entry_price\": entry_price,\n",
    "        \"stop_loss\": stop_loss,\n",
    "        \"take_profit\": take_profit,\n",
    "        \"position_size\": position_size\n",
    "    }\n",
    "\n",
    "results = {}\n",
    "for ticker in ticker_data:\n",
    "    results[ticker] = analyze_ticker(ticker_data[ticker], ticker, anchor_dates_dict[ticker])\n",
    "\n",
    "results_df = pd.DataFrame(results).T\n",
    "\n",
    "results_df.to_excel(\"anchored_vwap.xlsx\", index=True)\n",
    "\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f76410b-16d7-4b1d-b12e-26270921a9df",
   "metadata": {},
   "source": [
    "**Explanation**\n",
    "\n",
    "This script computes and plots Anchored VWAPs for each ticker, then applies a VWAP-based investing strategy. It calculates three anchored VWAP levelsyear-start, recent minimum, and recent maximumand uses them to determine market trend and generate investing signals. A bullish trend is identified when the current close is above both the year-start and last-min VWAPs, while a bearish trend occurs when the close is below both the year-start and last-max VWAPs. Entry signals are triggered when the price is near key VWAPs within 0.5 ATR, and signal exits are defined using ATR-based stop losses and VWAP-based take profits. It outputs a structured dictionary with investing details and saves annotated charts for each ticker, turning the VWAP strategy into a fully operational, risk-managed system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1cfe41-0880-41e3-b409-0a711646ab02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "print(\"\\nTrade Summary:\")\n",
    "for ticker, res in results.items():\n",
    "    print(f\"\\nTicker: {ticker} on {datetime.now().date()}\")\n",
    "    print(f\"Trend: {res['trend']}\")\n",
    "    print(f\"Signal: {res['signal']}\")\n",
    "    print(f\"Last Close: {res['last_close']:.2f}\")\n",
    "    print(f\"VWAPs - Year: {res['vwap_year']:.2f}, Min: {res['vwap_min']:.2f}, Max: {res['vwap_max']:.2f}\")\n",
    "    print(f\"ATR: {res['atr']:.2f}\")\n",
    "    if res[\"signal\"]:\n",
    "        print(f\"Entry: {res['entry_price']:.2f}, Stop Loss: {res['stop_loss']:.2f}, \"\n",
    "              f\"Take Profit: {res['take_profit']:.2f}\")\n",
    "        print(f\"Position Size: {res['position_size']:.2f} shares\")\n",
    "        print(f\"Executing {res['signal']} trade for {ticker}\")\n",
    "    else:\n",
    "        print(\"No trade signal generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca543f7-aaa5-45be-96a8-59f102e79e5f",
   "metadata": {},
   "source": [
    "**Explanation**\n",
    "\n",
    "This step runs the VWAP-based strategy and prints a clear, date-stamped investing summary for each. For every ticker, it displays the detected trend momentum, signal (if any), last closing price, VWAP levels (year, min, and max), and ATR value. If an investing signal is generated, it also shows detailed trade parameters including entry price, stop loss, take profit, and calculated position size. The output provides a concise and actionable snapshot of investing setups, supporting informed execution and ongoing strategy monitoring.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60bfb47-d5fd-4e80-bb76-1fafa94bf68f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from draw_avg import draw_5_days_avg\n",
    "from price_volume import draw_profile_of_data\n",
    "from vwaps_plot import vwaps_plot_build_save\n",
    "from misc.chart_annotation import get_chart_annotation_1d\n",
    "from ratio import draw_ratio   \n",
    "import matplotlib.pyplot as plt\n",
    "import warnings \n",
    "\n",
    "for ticker in ticker_data:\n",
    "    draw_5_days_avg(ticker=ticker, interval=\"15m\")  \n",
    "    print(f\"{ticker}: 5-day SMA image generated\")\n",
    "\n",
    "    draw_profile_of_data(ohlc_df=ticker_data[ticker], ticker=ticker)\n",
    "    print(f\"{ticker}: Price and Volume profile image generated\")\n",
    "\n",
    "GENERATE_INTRADAY_VWAP = False # We can toggle this to true if we want to re-enable it \n",
    "    \n",
    "intraday_df = get_ohlc_from_yf(ticker=ticker, period=\"5d\", interval=\"1m\")\n",
    "intraday_df = add_atr_col_to_df(intraday_df, n=ATR_SMOOTHING_N, exponential=False)\n",
    "\n",
    "if GENERATE_INTRADAY_VWAP:\n",
    "    vwaps_plot_build_save(\n",
    "        input_df=intraday_df,\n",
    "        anchor_dates=anchor_dates_dict[ticker],\n",
    "        chart_title=f\"{ticker} 1m with Anchored VWAPs\",\n",
    "        chart_annotation_func=get_chart_annotation_1d,\n",
    "        add_last_min_max=False,\n",
    "        file_name=f\"intraday_{ticker}.png\",\n",
    "        hide_extended_hours=True,\n",
    "        print_df=False\n",
    "    )\n",
    "    print(f\"{ticker}: Intraday VWAP image generated\") \n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046848cb-42a0-48f7-a71d-18e680ca84ae",
   "metadata": {},
   "source": [
    "**Explanation**\n",
    "\n",
    "The reason for integrating intraday tools such as SMA, volume profiles, relative strength ratios, and quick charting features is to enrich the daily investing strategy with more granular, real-time insights into market conditions. By incorporating these elements, investors can monitor price movements and momentum trends more effectively within the day, allowing for better short-term decision-making. The use of intraday VWAP and other indicators helps identify key price levels and momentum trends during trading hours, enhancing the strategys accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc3258a-3368-4131-8558-fb8a9c10f96a",
   "metadata": {},
   "source": [
    "### EXPONENTIAL MOVING AVERAGE CROSSOVER (MOMENTUM)\n",
    "\n",
    "The EWMAC (Exponentially Weighted Moving Average Crossover) strategy is an investing rule that captures trends in asset prices using only price data. It compares fast and slow EWMA of the price to detect trends: when the fast EWMA is above the slow EWMA, it signals an uptrend (go long), and when the fast EWMA is below the slow EWMA, it signals a downtrend (go short). The raw signal is adjusted for volatility and scaled to reflect forecast strength, then capped to limit extremes. This simple systematic approach is backed by both empirical performance and behavioral finance theory, making it a robust and explainable investing strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594a3df1-4735-4d9a-8b59-0be9daf7dc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "tickers = [\"ABG.JO\", \"ADH.JO\", \"AEL.JO\", \"AFE.JO\", \"AFH.JO\", \"AFT.JO\", \"AGL.JO\", \"AHR.JO\", \"AIP.JO\", \"ANG.JO\", \"ANH.JO\", \"APN.JO\", \"ARI.JO\",\n",
    "          \"ARL.JO\", \"ATT.JO\", \"AVI.JO\", \"BAW.JO\", \"BHG.JO\", \"BID.JO\", \"BLU.JO\", \"BOX.JO\", \"BTI.JO\", \"BTN.JO\", \"BVT.JO\", \"BYI.JO\", \"CFR.JO\", \"CLS.JO\",\n",
    "          \"CML.JO\", \"COH.JO\", \"CPI.JO\", \"CSB.JO\", \"DCP.JO\", \"DRD.JO\", \"DSY.JO\", \"DTC.JO\", \"EMI.JO\", \"EQU.JO\", \"EXX.JO\", \"FBR.JO\", \"FFB.JO\", \"FSR.JO\",\n",
    "          \"FTB.JO\", \"GFI.JO\", \"GLN.JO\", \"GND.JO\", \"GRT.JO\", \"HAR.JO\", \"HCI.JO\", \"HDC.JO\", \"HMN.JO\", \"HYP.JO\", \"IMP.JO\", \"INL.JO\", \"INP.JO\", \"ITE.JO\",\n",
    "          \"JSE.JO\", \"KAP.JO\", \"KIO.JO\", \"KRO.JO\", \"KST.JO\", \"LHC.JO\", \"LTE.JO\", \"MCG.JO\", \"MKR.JO\", \"MNP.JO\", \"MRP.JO\", \"MSP.JO\", \"MTH.JO\", \"MTM.JO\",\n",
    "          \"MTN.JO\", \"N91.JO\", \"NED.JO\", \"NPH.JO\", \"NPN.JO\", \"NRP.JO\", \"NTC.JO\", \"NY1.JO\", \"OCE.JO\", \"OMN.JO\", \"OMU.JO\", \"OUT.JO\", \"PAN.JO\", \"PHP.JO\",\n",
    "          \"PIK.JO\", \"PMR.JO\", \"PPC.JO\", \"PPH.JO\", \"PRX.JO\", \"QLT.JO\", \"RBX.JO\", \"RCL.JO\", \"RDF.JO\", \"REM.JO\", \"RES.JO\", \"RLO.JO\", \"RNI.JO\", \"S32.JO\",\n",
    "          \"SAC.JO\", \"SAP.JO\", \"SBK.JO\", \"SHC.JO\", \"SHP.JO\", \"SLM.JO\", \"SNT.JO\", \"SOL.JO\", \"SPG.JO\", \"SPP.JO\", \"SRE.JO\", \"SRI.JO\", \"SSS.JO\",\n",
    "          \"SSU.JO\", \"SSW.JO\", \"SUI.JO\", \"TBS.JO\", \"TFG.JO\", \"TGA.JO\", \"TKG.JO\", \"TRU.JO\", \"TSG.JO\", \"VAL.JO\", \"VKE.JO\", \"VOD.JO\", \"WBC.JO\", \"WHL.JO\"]\n",
    "\n",
    "start_date = \"2024-01-01\"\n",
    "end_date = datetime.today().strftime('%Y-%m-%d')\n",
    "Lfast = 16\n",
    "Lslow = 4 * Lfast\n",
    "vol_lookback = 25\n",
    "capmin = -20\n",
    "capmax = 20\n",
    "\n",
    "def ewmac_forecast_scalar(Lfast, Lslow):\n",
    "    return 10 / np.sqrt(Lfast)\n",
    "\n",
    "def retry_download(ticker, start, end, max_retries=3):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            df = yf.download(ticker, start=start, end=end)\n",
    "            if not df.empty:\n",
    "                return df\n",
    "        except Exception as e:\n",
    "            print(f\"[Retry {attempt+1}] Error downloadding {ticker}: {e}\")\n",
    "    print(f\"Failed to retrieve data for {ticker} after {max_retries} attempts.\")\n",
    "    return pd.DataFrame()\n",
    "    \n",
    "\n",
    "f_scalar = ewmac_forecast_scalar(Lfast, Lslow)\n",
    "\n",
    "data = yf.download(tickers, start=start_date, end=end_date)\n",
    "\n",
    "for ticker in tickers:\n",
    "    try:\n",
    "        if ticker not in data[\"Close\"] or data[\"Close\"][ticker].dropna().empty:\n",
    "            print(f\"No data for {ticker} in batch. Retrying individually...\")\n",
    "            single_data = retry_download(ticker, start_date, end_date)\n",
    "            price = single_data[\"Close\"].dropna()\n",
    "        else:\n",
    "            price = data[\"Close\"][ticker].dropna()\n",
    "\n",
    "        if price.empty:\n",
    "            print(f\"No data for {ticker}. Skipping...\")\n",
    "            continue\n",
    "            \n",
    "        fast_ewma = price.ewm(span=Lfast).mean()\n",
    "        slow_ewma = price.ewm(span=Lslow).mean()\n",
    "        raw_ewmac = fast_ewma - slow_ewma\n",
    "        returns = price.pct_change()\n",
    "        vol = returns.ewm(span=vol_lookback).std()\n",
    "        vol_adj_ewmac = raw_ewmac / vol\n",
    "        forecast = vol_adj_ewmac * f_scalar\n",
    "        cap_forecast = forecast.clip(lower=capmin, upper=capmax)\n",
    "\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(18, 6))\n",
    "        axs[0].plot(price, label='Price', color='black')\n",
    "        axs[0].plot(fast_ewma, label=f'Fast EWMA ({Lfast})', linestyle='--')\n",
    "        axs[0].plot(slow_ewma, label=f'Slow EWMA ({Lslow})', linestyle='--')\n",
    "        axs[0].set_title(f\"EWMAC Crossover\\n{ticker}\")\n",
    "        axs[0].set_xlabel(\"Date\")\n",
    "        axs[0].set_ylabel(\"Price\")\n",
    "        axs[0].legend()\n",
    "        axs[0].grid(True)\n",
    "        \n",
    "        axs[1].plot(cap_forecast, label='Capped Forecast Signal', color='blue')\n",
    "        axs[1].axhline(10, color='green', linestyle='--', label='Buy Threshold')\n",
    "        axs[1].axhline(-10, color='red', linestyle='--', label='Sell Threshold')\n",
    "        axs[1].set_title(\"Capped EWMAC Forecast Signal\")\n",
    "        axs[1].set_xlabel(\"Date\")\n",
    "        axs[1].set_ylabel(\"Forecast Value\")\n",
    "        axs[1].legend()\n",
    "        axs[1].grid(True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{ticker}_ewmac_combined.png\")\n",
    "        plt.close()\n",
    "\n",
    "        print(f\" Saved: {ticker}_ewmac_combined.png\")\n",
    "\n",
    "        time.sleep(0.2)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" Error with {ticker}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f6a290-50a9-4af3-8580-12f22d280675",
   "metadata": {},
   "source": [
    "**Explanation**\n",
    "\n",
    "The Exponentially Weighted Moving Average Crossover (EWMAC) strategy is a robust and intuitive momentum investing rule that captures medium- to long-term momentum in asset prices. By comparing a fast-moving average to a slow-moving average, the strategy identifies directional trends: it generates buy signals when prices are trending upward (fast MA > slow MA) and sell signals during downtrends (fast MA < slow MA).\n",
    "\n",
    "The result is a dynamic signal that is responsive to trends, adaptive to volatility, and simple to implement, making it an ideal component of a systematic investing strategy. Its strength lies not only in its performance but also in its behavioral justification, simplicity, and positive skewness  offering large potential gains during strong market trends while limiting losses in range-bound periods."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
